version: "3"

services:
  sleek-airflow:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    volumes:
      - ./airflow:/opt/airflow
      - ./workspace:/opt/workspace
    ports:
      - "8080:8080"
    environment:
      MLFLOW_TRACKING_URI: "http://0.0.0.0:5000"
      BACKEND_STORE_URI: "postgresql://mlflow:mlflow@mlflow-db:5432/mlflow"
      ARTIFACT_STORE_URI: "s3://mlflow-artifacts"
      AWS_ACCESS_KEY_ID: "minioadmin"
      AWS_SECRET_ACCESS_KEY: "minioadmin"
      MLFLOW_S3_ENDPOINT_URL: "http://minio:9000"
    networks:
      - spark-cluster
    depends_on:
      - spark-master
    command: bash -c "rm -f /opt/airflow/airflow-webserver.pid && airflow db init && (airflow scheduler & airflow webserver)"

  spark-master:
    image: bitnami/spark:latest
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
    ports:
      - "8081:8080" # Web UI for Spark Master
      - "7077:7077" # Spark Master Port for worker connections
    networks:
      - spark-cluster
    volumes:
      - ./workspace:/opt/workspace

  spark-worker-1:
    build:
      context: .
      dockerfile: Dockerfile.spark-worker
    container_name: spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER=spark://spark-master:7077
    networks:
      - spark-cluster
    volumes:
      - ./workspace:/opt/workspace
    depends_on:
      - spark-master

  spark-worker-2:
    build:
      context: .
      dockerfile: Dockerfile.spark-worker
    container_name: spark-worker-2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER=spark://spark-master:7077
    networks:
      - spark-cluster
    volumes:
      - ./workspace:/opt/workspace
    depends_on:
      - spark-master

  minio:
    image: quay.io/minio/minio:latest
    container_name: minio
    command: server /data --console-address ":9001"
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin
    ports:
      - "9000:9000" # MinIO API
      - "9001:9001" # MinIO Console
    networks:
      - spark-cluster
    volumes:
      - minio-data:/data

  mlflow:
    image: mlflow/mlflow:latest
    container_name: mlflow-server
    ports:
      - "5000:5000"
    environment:
      MLFLOW_TRACKING_URI: "http://0.0.0.0:5000"
      BACKEND_STORE_URI: "postgresql://mlflow:mlflow@mlflow-db:5432/mlflow"
      ARTIFACT_STORE_URI: "s3://mlflow-artifacts"
      AWS_ACCESS_KEY_ID: "minioadmin"
      AWS_SECRET_ACCESS_KEY: "minioadmin"
      MLFLOW_S3_ENDPOINT_URL: "http://minio:9000"
    build:
      context: .
      dockerfile: Dockerfile.mlflow
    depends_on:
      - mlflow-db
      - minio
    networks:
      - spark-cluster

  mlflow-db:
    image: postgres:13
    container_name: mlflow-db
    environment:
      POSTGRES_USER: mlflow
      POSTGRES_PASSWORD: mlflow
      POSTGRES_DB: mlflow
    ports:
      - "5432:5432"
    volumes:
      - ./postgres_data:/var/lib/postgresql/data
    networks:
      - spark-cluster

  minio-client:
    image: minio/mc
    depends_on:
      - minio
    entrypoint: >
      /bin/sh -c "
      until (/usr/bin/mc alias set mlflow http://minio:9000 minioadmin minioadmin) do echo 'waiting for minio'; sleep 5; done;
      /usr/bin/mc mb -p mlflow/mlflow-artifacts;
      /usr/bin/mc policy set public mlflow/mlflow-artifacts;
      "
    networks:
      - spark-cluster

  model-deploy:
    build:
      context: .
      dockerfile: Dockerfile.fastapi
    ports:
      - "5555:5555"
    volumes:
      - ./model-deploy/app:/app/app
    networks:
      - spark-cluster

volumes:
  shared-workspace:
  minio-data:
  postgres_data:

networks:
  spark-cluster:
    driver: bridge
